# -*- coding: utf-8 -*-
"""process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GDGsKrfNPedM0Pry5dvKdh7dAwM11tR5
"""

import os
import pickle
import numpy as np
import re
from tqdm import tqdm
from PIL import Image

import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Concatenate, Layer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Import necessary libraries for BLEU score
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu

# ------------ USER PATHS --------------
ZIP_PATH = "/content/drive/MyDrive/Images"           # Folder with images (not zipped)
CAPTION_FILE = "/content/captions.txt"  # Captions file
FEATURES_FILE = "/content/drive/MyDrive/features.pkl"
TOKENIZER_FILE = "/content/drive/MyDrive/tokenizer.pkl"
MODEL_PATH = "/content/drive/MyDrive/caption_model.h5"

# ------------ 1. Extract or Load Features --------------

def extract_image_id(filename):
    return os.path.splitext(os.path.basename(filename))[0]

if not os.path.exists(FEATURES_FILE):
    print("Extracting features from images folder...")
    base_model = InceptionV3(weights='imagenet')
    model_incep = Model(inputs=base_model.input, outputs=base_incep.layers[-2].output)

    features = {}
    image_files = [f for f in os.listdir(ZIP_PATH) if f.lower().endswith('.jpg')]
    print(f"Total images found: {len(image_files)}")
    for file_name in tqdm(image_files):
        img_path = os.path.join(ZIP_PATH, file_name)
        img = Image.open(img_path).convert('RGB')
        img = img.resize((299, 299))
        image = img_to_array(img)
        image = np.expand_dims(image, axis=0)
        image = preprocess_input(image)
        feature = model_incep.predict(image, verbose=0)
        img_id = extract_image_id(file_name)
        features[img_id] = feature
    with open(FEATURES_FILE, 'wb') as f:
        pickle.dump(features, f)
    print(f"Saved features for {len(features)} images.")
else:
    print(f"Loading existing features from {FEATURES_FILE}...")
    with open(FEATURES_FILE, 'rb') as f:
        features = pickle.load(f)
    print(f"Loaded features for {len(features)} images.")

# ------------ 2. Load and Process Captions -------------

mapping = {}
print(f"Reading captions from: {CAPTION_FILE}")
with open(CAPTION_FILE, 'r') as f:
    captions_doc = f.read()

lines = captions_doc.strip().split('\n')
print(f"Total lines in captions file: {len(lines)}")

for line in lines:
    if len(line) < 2 or ',' not in line:
        continue
    image_id, caption = line.split(',', 1)
    image_id = image_id.strip()
    caption = caption.strip()
    image_id = image_id.split('.')[0]
    if image_id not in mapping:
        mapping[image_id] = []
    mapping[image_id].append(caption)

print(f"Unique image IDs in captions: {len(mapping)}")

def clean_captions(mapping):
    for key, caps in mapping.items():
        for i in range(len(caps)):
            caption = caps[i].lower()
            caption = re.sub(r'[^a-z ]', '', caption)
            caption = re.sub(r'\s+', ' ', caption).strip()
            caps[i] = 'startseq ' + ' '.join([w for w in caption.split() if len(w) > 1]) + ' endseq'

clean_captions(mapping)
print("Captions cleaned.")

caption_ids = set(mapping.keys())
feature_ids = set(features.keys())

print(f"Number of caption image IDs: {len(caption_ids)}")
print(f"Number of feature image IDs: {len(feature_ids)}")

# Add print statements to help diagnose the mismatch
print("\nFirst 10 Feature Image IDs:", list(feature_ids)[:10])
print("First 10 Caption Image IDs:", list(caption_ids)[:10])


common_ids = caption_ids.intersection(feature_ids)
print(f"Number of common IDs: {len(common_ids)}")

if len(common_ids) == 0:
    raise ValueError("No common image IDs between captions and features! Check dataset and filenames.")

filtered_mapping = {img_id: mapping[img_id] for img_id in common_ids}
image_ids = list(filtered_mapping.keys())

all_captions = [cap for caps in filtered_mapping.values() for cap in caps]

# ------------ 3. Create or Load Tokenizer -------------

if not os.path.exists(TOKENIZER_FILE):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(all_captions)
    with open(TOKENIZER_FILE, 'wb') as f:
        pickle.dump(tokenizer, f)
    print(f"Tokenizer created and saved to {TOKENIZER_FILE}")
else:
    with open(TOKENIZER_FILE, 'rb') as f:
        tokenizer = pickle.load(f)
    print("Tokenizer loaded.")

vocab_size = len(tokenizer.word_index) + 1
max_length = max(len(c.split()) for c in all_captions)

print(f"Vocabulary size: {vocab_size}")
print(f"Max caption length: {max_length}")

# ------------ 4. Train/Validation Split -------------

train_ids, val_ids = train_test_split(image_ids, test_size=0.1, random_state=42)
print(f"Training samples: {len(train_ids)}, Validation samples: {len(val_ids)}")

# ------------ 5. Data Generator -------------

batch_size = 64

def data_generator(image_ids, mapping, features, tokenizer, max_length, vocab_size, batch_size):
    X1, X2, y = [], [], []
    n = 0
    while True:
        for img_id in image_ids:
            caps = mapping[img_id]
            for cap in caps:
                seq = tokenizer.texts_to_sequences([cap])[0]
                for i in range(1, len(seq)):
                    in_seq, out_seq = seq[:i], seq[i]
                    in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]

                    X1.append(features[img_id][0])
                    X2.append(in_seq)
                    y.append(out_seq)
                    n += 1

                    if n == batch_size:
                        yield (np.array(X1), np.array(X2)), np.array(y)
                        X1, X2, y = [], [], []
                        n = 0

# ------------ 6. Bahdanau Attention -------------

class BahdanauAttention(Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = Dense(units)
        self.W2 = Dense(units)
        self.V = Dense(1)

    def call(self, query, values):
        query_with_time_axis = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

# ------------ 7. Build the Model -------------

units = 256

inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(units, activation='relu')(fe1)

inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, units, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = LSTM(units, return_sequences=True)(se2)

attention = BahdanauAttention(units)
context_vector, attention_weights = attention(fe2, se3)

decoder1 = Concatenate(axis=-1)([fe2, context_vector])
decoder2 = Dense(units, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

print(model.summary())

# ------------ 8. Wrap Generators into tf.data.Dataset -------------

output_signature = (
    (
        tf.TensorSpec(shape=(None, 2048), dtype=tf.float32),
        tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),
    ),
    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)
)

train_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(train_ids, filtered_mapping, features, tokenizer, max_length, vocab_size, batch_size),
    output_signature=output_signature,
).prefetch(tf.data.AUTOTUNE)

val_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(val_ids, filtered_mapping, features, tokenizer, max_length, vocab_size, batch_size),
    output_signature=output_signature,
).prefetch(tf.data.AUTOTUNE)

steps_per_epoch = max(len(train_ids) * 5 // batch_size, 1)  # approx 5 captions/image
validation_steps = max(len(val_ids) * 5 // batch_size, 1)

# ------------ 9. Train -------------

checkpoint = ModelCheckpoint(MODEL_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)

print("Starting model training...")
history = model.fit(
    train_dataset,
    epochs=15,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[checkpoint, earlystop]
)
print("Model training finished.")

# ------------ 10. Evaluate with BLEU Score (on validation set) -------------

print("\nCalculating BLEU score on the validation set...")

# ------------ Helper function for BLEU evaluation ------------

# Map predicted index back to a word
def idx_to_word(integer, tokenizer):
    return next((word for word, index in tokenizer.word_index.items() if index == integer), None)

# Function to generate caption for BLEU scoring
def generate_caption_for_bleu(model, tokenizer, photo, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')
        yhat = model.predict([photo, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = idx_to_word(yhat, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break
    return in_text.replace("startseq", "").replace("endseq", "").split()  # return as list of words

# ------------ BLEU score calculation ------------
references = []
candidates = []

for img_id in tqdm(val_ids):
    if img_id in features and img_id in filtered_mapping:
        # Reference captions
        reference_captions = [
            cap.replace("startseq", "").replace("endseq", "").strip().split()
            for cap in filtered_mapping[img_id]
        ]
        references.append(reference_captions)

        # Candidate caption
        candidate_caption = generate_caption_for_bleu(model, tokenizer, features[img_id], max_length)
        candidates.append(candidate_caption)
    else:
        print(f"Warning: Skipping BLEU calculation for {img_id} due to missing data.")

if references and candidates and len(references) == len(candidates):
    print("Calculating BLEU scores...")
    bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))
    bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0))
    bleu3 = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0))
    bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25))

    print(f"BLEU-1: {bleu1:.4f}")
    print(f"BLEU-2: {bleu2:.4f}")
    print(f"BLEU-3: {bleu3:.4f}")
    print(f"BLEU-4: {bleu4:.4f}")
else:
    print("Skipping BLEU score calculation: No valid references or candidates found.")

# Generate captions for validation images and collect references
for img_id in tqdm(val_ids):
    if img_id in features and img_id in filtered_mapping:
        # Get reference captions (cleaned and split into words)
        reference_captions = [cap.replace("startseq", "").replace("endseq", "").strip().split() for cap in filtered_mapping[img_id]]
        references.append(reference_captions)

        # Generate candidate caption (split into words)
        candidate_caption = generate_caption_for_bleu(model, tokenizer, features[img_id], max_length)
        candidates.append(candidate_caption)
    else:
        print(f"Warning: Skipping BLEU calculation for {img_id} due to missing data.")


if references and candidates and len(references) == len(candidates):
    # Calculate BLEU scores
    # weights=(1, 0, 0, 0) for BLEU-1, (0.5, 0.5, 0, 0) for BLEU-2, etc.
    print("Calculating BLEU scores...")
    bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))
    bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0))
    bleu3 = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0))
    bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25))

    print(f"BLEU-1: {bleu1:.4f}")
    print(f"BLEU-2: {bleu2:.4f}")
    print(f"BLEU-3: {bleu3:.4f}")
    print(f"BLEU-4: {bleu4:.4f}")
else:
    print("Skipping BLEU score calculation: No valid references or candidates found.")

from google.colab import output
from PIL import Image
from io import BytesIO
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.applications.inception_v3 import InceptionV3

# Global variable to store uploaded image
uploaded_img = None
model_incep = None  # will hold InceptionV3 model for feature extraction

def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def upload_image(file_bytes):
    global uploaded_img
    uploaded_img = Image.open(BytesIO(bytearray(file_bytes))).convert('RGB')
    return "Image received!"

def generate_caption():
    global uploaded_img, model, tokenizer, max_length, model_incep

    if uploaded_img is None:
        return "No image uploaded yet!"

    img_resized = uploaded_img.resize((299, 299))
    image_array = img_to_array(img_resized)
    image_array = np.expand_dims(image_array, axis=0)
    image_array = preprocess_input(image_array)

    if model_incep is None:
        base_model = InceptionV3(weights='imagenet')
        model_incep = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)

    image_features = model_incep.predict(image_array, verbose=0)

    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')
        yhat = model.predict([image_features, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = idx_to_word(yhat, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break

    caption = in_text.replace("startseq", "").replace("endseq", "").strip()
    return caption

output.register_callback('notebook.upload_image', upload_image)
output.register_callback('notebook.generate_caption', generate_caption)

from IPython.display import display, HTML

display(HTML('''
<style>
  body {
    font-family: 'Comic Sans MS', cursive, sans-serif;
    background: linear-gradient(135deg, #ff9a9e 0%, #fad0c4 100%);
    color: #333;
    text-align: center;
    padding: 30px;
  }
  h2 {
    font-size: 2.5rem;
    margin-bottom: 20px;
    color: #5a2a27;
    text-shadow: 1px 1px 2px #fff3;
  }

  .polaroid {
    background: white;
    width: 320px;
    padding: 15px 15px 40px 15px;
    margin: auto;
    border-radius: 10px;
    box-shadow: 3px 6px 15px rgba(0,0,0,0.3);
    position: relative;
    font-weight: bold;
    color: #5a2a27;
  }

  .polaroid img {
    width: 100%;
    border-radius: 7px;
    box-shadow: 0 8px 15px rgba(0,0,0,0.2);
    margin-bottom: 15px;
  }

  #fileInput {
    margin-top: 10px;
    cursor: pointer;
    font-weight: 600;
    color: #5a2a27;
    border: 2px solid #5a2a27;
    border-radius: 5px;
    padding: 10px 15px;
    background: #fff0db;
    transition: all 0.3s ease;
  }
  #fileInput:hover {
    background: #f8d9a3;
  }

  #generate-btn {
    background: #ff6f91;
    border: none;
    padding: 10px 25px;
    font-size: 1.2rem;
    border-radius: 25px;
    cursor: pointer;
    color: white;
    box-shadow: 0 5px 15px rgba(255,111,145,0.5);
    transition: background 0.3s ease;
    margin-top: 10px;
  }
  #generate-btn:disabled {
    background: #d8d8d8;
    cursor: not-allowed;
    box-shadow: none;
  }
  #generate-btn:hover:not(:disabled) {
    background: #ff517d;
  }

  #caption-output {
    margin-top: 20px;
    font-size: 1.2rem;
    min-height: 40px;
    font-family: 'Courier New', Courier, monospace;
    color: #7b3e3e;
  }

</style>

<div class="polaroid">
  <h2> Upload Your Image</h2>
  <input type="file" id="fileInput" accept="image/*" />
  <img id="preview" src="" style="display:none;" />
  <br/>
  <button id="generate-btn" disabled>Generate Caption</button>
  <div id="caption-output"></div>
</div>

<script>
  const input = document.getElementById('fileInput');
  const preview = document.getElementById('preview');
  const generateBtn = document.getElementById('generate-btn');
  const captionOutput = document.getElementById('caption-output');

  input.onchange = evt => {
    const [file] = input.files;
    if (file) {
      preview.src = URL.createObjectURL(file);
      preview.style.display = 'block';
      generateBtn.disabled = false;
      captionOutput.textContent = '';

      const reader = new FileReader();
      reader.onload = function() {
        const arrayBuffer = reader.result;
        const bytes = new Uint8Array(arrayBuffer);
        google.colab.kernel.invokeFunction('notebook.upload_image', [Array.from(bytes)], {});
      };
      reader.readAsArrayBuffer(file);
    }
  };

  generateBtn.onclick = () => {
    captionOutput.textContent = 'Generating caption... ';

    google.colab.kernel.invokeFunction('notebook.generate_caption', [], {}).then(result => {
      captionOutput.textContent = result.data['text/plain'].replace(/'/g, "");
    }).catch(() => {
      captionOutput.textContent = 'Error generating caption ';
    });
  };
</script>
'''))

